# Infra DDD Review ExecPlan

This ExecPlan is a living document obeying docs/AI_context/PLANS.md and referencing the DDD review checklist in docs/AI_context/test_context.md. It captures the guardrail observations for every class under src/*/infra on 2026-01-27 and lays out the unit tests and comment reminders that must exist before any code change. Save this file as docs/prompts/infra_review_exec_plan.md and update Progress, Surprises & Discoveries, and the living sections should new insights appear.

## Purpose / Big Picture

This review ensures the infrastructure layer stays consistent with the aggregate boundaries, invariants, context-map constants, dependency-direction rules, and entity/value-object roles defined in docs/AI_context/test_context.md. After the plan is implemented, the team will have explicit test coverage for each invariant in the infra repositories (Dataset keymaps, DuckDB bootstrap/migrations, Bronze persistence, logging, and UI helpers) and will also have docstrings explaining why those invariants exist so later reviewers can trust them. The plan drives toward the observable outcome that `python -m pytest tests/unit/infra` runs cleanly against these new tests and that the codebase still obeys the Bronze/Silver/Gold contracts before any production changes.

## Progress

- [x] (2026-01-27 17:05Z) Captured every Python class under src/*/infra, annotated its aggregate boundary + invariants, and linked it to the DDD checklist.
- [x] (2026-01-27 17:25Z) Converted those observations into infra-focused pytest modules plus the recommended docstrings/comments (dataset keymaps, ResultFileAdapter, BronzeFileWriter, DuckDB infra, logger, chart adapters).
- [x] (2026-01-27 17:55Z) Added the supporting dependencies (httpx, altair, pandas), alias packages (`models.ops_models`, `ui_settings`), and the updated `RunResult`/`LoggerFactory` behaviors, then executed the infra pytest suite (`test_dataset_service`, `test_dataset_value_objects`, `test_result_file_adapter`, `test_bronze_file_writer`, `test_logger_factory`, `test_duckdb_bootstrap`, `test_migrations_service`, `test_duckdb_ops_repo`, `test_ui_duckdb_ops_repo`, `test_chart_adapters`) to prove the invariants hold.

## Surprises & Discoveries

- Observation: `DatasetService` acts as the aggregate root for dataset metadata, validating the YAML file at `Folders.dataset_keymap_absolute_path()/DATASET_KEYMAP_FILENAME`, forcing version/domain/source/dataset/ticker_scope/key_cols constraints, deduplicating on `DatasetKeymapEntry.identity_key`, and optionally enforcing parity with `DTO_REGISTRY`. It therefore owns the context map constants for dataset definitions.  Evidence: `src/data_layer/infra/dataset/dataset_service.py` raises ValueError on missing fields, restricts `ticker_scope` to `{"per_ticker","global"}`, and runs `_validate_dto_registry`.  Proposed tests: 1) Create a temporary YAML lacking `ticker_scope` or with `ticker_scope: bad` and assert `load_dataset_keymap` raises. 2) Prepare YAML with duplicate domain/source/dataset/discriminator/ticker_scope entries and assert the duplicate identity triggers ValueError. 3) Patch `data_layer.dtos.dto_registry.DTO_REGISTRY` to exclude one dataset, set `STRICT_DTO_REGISTRY=1`, and confirm `load_dataset_keymap` raises the registry mismatch error, then rerun with `STRICT_DTO_REGISTRY` unset to ensure only a warning is logged. 4) Load a well-formed keymap and assert each returned `DatasetKeymapEntry` matches the YAML fields.  Suggested comment: Document at the top of `DatasetService` (and/or `DatasetKeymap`) that this class is the canonical source for dataset identities and that any addition to the DTO registry must be reflected here to keep Bronze→Silver contracts aligned.
- Observation: `DatasetKeymapEntry` and `DatasetKeymap` are immutable value objects bootstrapping the identity-match invariant exploited by the ingestion services; `matches_identity` gates per-ticker vs global coverage and `require` raises when no match exists.  Evidence: `src/data_layer/infra/dataset/dataset_keymap_entry.py` checks `ticker_scope` and coerces `identity.discriminator` to blank, `DatasetKeymap.require` raises `KeyError` when find returns None.  Proposed tests: 1) Assert `matches_identity` returns True for a global entry when `identity.ticker` is empty and False when a ticker is present. 2) Assert `matches_identity` returns True for a per-ticker entry only when a ticker exists. 3) Confirm `DatasetKeymap.require` raises `KeyError` when the identity has no mapping.  Suggested comment: Annotate `DatasetKeymapEntry.identity_key` to mention that the string is the deduplication key used by the Service to reject conflicting definitions.
- Observation: `DatasetIdentity` and `DatasetWatermark` are pure value objects that serialize domain/source/dataset/discriminator/ticker plus coverage windows into the canonical watermark string consumed by `ops.file_ingestions`.  Evidence: `DatasetIdentity.serialize_watermark` and `DatasetWatermark.serialize` build strings like `domain|source|dataset|discriminator|ticker@coverage_from=...;coverage_to=...`.  Proposed tests: 1) Confirm `serialize_watermark` renders the expected string when all fields are present and uses empty tokens when coverage dates are None. 2) Confirm `DatasetWatermark.serialize` simply proxies to the identity serialization and that the string matches the format expected by `DuckDbOpsRepo.load_input_watermarks`.  Suggested comment: Add a docstring to `DatasetWatermark` highlighting that the serialization result is the ground truth watermark fed into the Gold promotion machinery.
- Observation: `ResultFileAdapter` is the infrastructure bridge that reads/writes Bronze JSON, rehydrates `DatasetRecipe`/`RunRequest`, tolerates missing fields, and provides helpers (`files_to_date_dict`, `latest_date`, `get_tuple_from_content`) for downstream services.  Evidence: `src/data_layer/infra/result_file_adaptor.py` writes to `result.request.bronze_absolute_filename`, logs warnings when rehydration fails, and `_parse_now` falls back to `datetime.utcnow()` for bad strings.  Proposed tests: 1) Use a temporary `RunResult` (with request and recipe) to exercise `write`, then read the file and assert it contains the same dict plus that `read` returns a matching `RunResult`. 2) Persist a JSON file without a `request` block and assert `read` issues warnings but still returns a `RunResult` with `request=None` and default `status_code`. 3) Call `_parse_now` with an ISO string, a `datetime`, and an invalid string to confirm it returns the correct parsed value or UTC fallback. 4) Pass a mix of filenames to `files_to_date_dict` and confirm only those ending with `YYYY-MM-DD` (with optional `T...`) are included.  Suggested comment: Add a short note above `_read_run_result_payload` highlighting that Bronze files must remain append-only and that the parser intentionally tolerates missing/invalid nested objects so replay can resume.
- Observation: `BronzeFileWriter` enforces that a `RunResult` cannot be persisted unless it carries a `RunRequest` with a `bronze_relative_filename`, so Bronze files cannot be written for partial results.  Evidence: `src/data_layer/infra/bronze_file_writer.py` raises `ValueError` when `result` or `result.request` is falsy, serializes to JSON under `Folders.data_absolute_path()`.  Proposed tests: 1) Pass a `RunResult` whose `request` is `None` and assert `write` raises `ValueError`. 2) Provide a real request with `bronze_relative_filename`, invoke `write`, and confirm the JSON file exists under `Folders.data_absolute_path()` (using a temporary override for Folders) and contains the expected structured payload.  Suggested comment: Document on `write` that this guard enforces the Bronze append-only policy described in docs/AI_context/bronze_data_contracts.md.
- Observation: `LoggerFactory` centralizes logging so every infra component can reuse the same log directory, avoid duplicate handlers, and switch to INFO when `ENV=DEV`.  Evidence: `src/data_layer/infra/logger.py` uses `Folders.logs_absolute_path()`, sets `propagate=False`, and conditionally adds `StreamHandler`/`FileHandler` only if missing.  Proposed tests: 1) Set `ENV=DEV` and ensure `create_logger` returns a logger with level `INFO`, then set another value to confirm level `WARN`. 2) Call `create_logger` twice with the same name and assert only one `StreamHandler` and one `FileHandler` are attached. 3) Pass a custom `log_path` (e.g., a temporary directory) and assert the directory is created and the log file path lives under it.  Suggested comment: Note in the class docstring why `propagate` is False and why the file handler uses `delay=True`, so future contributors understand why duplicate appends are avoided.
- Observation: `DuckDbBootstrap` is the aggregate that owns the DuckDB connection, applies migrations once before use, exposes `transaction` with commit/rollback semantics, and reuses that connection for the ops/silver/gold wrappers to keep those aggregates consistent.  Evidence: `src/data_layer/infra/duckdb/duckdb_bootstrap.py` instantiates `MigrationsService`, tracks `_owns_connection`, sets `_migrations_applied`, and wraps `transaction`.  Proposed tests: 1) Inject a stubbed `MigrationsService` to assert `apply` runs only on the first `connect` call. 2) Use a fake DuckDB connection object (with `execute`, `commit`, and `rollback` spies) to drive `transaction` and confirm `BEGIN`/`COMMIT` happen on success and `ROLLBACK` happens when an exception is raised. 3) Confirm `ops_transaction`, `silver_transaction`, and `gold_transaction` all reuse the same base transaction and that nested usage does not reapply migrations.  Suggested comment: Document that `DuckDbBootstrap` is responsible for both creation and safe disposal of the underlying connection to remind people to call `close` when constructing their own bootstrap.
- Observation: `Migration`/`MigrationsService` together enforce the schema migration contract (`ops.schema_migrations` table) and forbid duplicate versions or checksum mismatches.  Evidence: `Migration.from_filename` splits `<version>_<name>`, computes SHA256, and `_apply_migration` inserts into `ops.schema_migrations`; `apply` sorts files, detects duplicates via `seen_versions`, and compares checksums with already applied migrations.  Proposed tests: 1) Create sample files `0001_init.sql` and `0001_init_extra.sql` and assert `from_filename` extracts the same version but different names, and the service rejects duplicates. 2) Provide a file with no underscore and assert `Migration.from_filename` raises ValueError. 3) Insert a row in `ops.schema_migrations` with a checksum that differs from the filesystem file and assert `apply` raises due to checksum mismatch. 4) Confirm `apply` logs a message and returns the list of versions it inserted when new migrations exist.  Suggested comment: Mention in `Migration` docstring that `TABLENAME` must stay aligned with `DuckDbOpsRepo` so both readers know they share the same metadata table.
- Observation: `DuckDbOpsRepo` under `data_layer.ops.infra` is the transactional aggregate that merges Bronze/Silver/Gold metadata into `ops.file_ingestions`, normalizes optional `discriminator`/`ticker`, and produces watermark strings for the promotion pipeline.  Evidence: `upsert_file_ingestion` uses `MERGE` keyed on `run_id`/`file_id`, `get_latest_*` queries with `COALESCE`, `list_promotable_file_ingestions` filters `bronze_can_promote`, `load_input_watermarks` serializes `DatasetWatermark`.  Proposed tests: 1) Use an in-memory DuckDB database to call `upsert_file_ingestion` twice with the same `run_id`/`file_id` and assert the row is updated rather than duplicated. 2) Insert rows with `discriminator=None`/`ticker=None` and assert `get_latest_bronze_to_date` and `get_latest_silver_to_date` return the same date. 3) Seed promotable rows (bronze rows with `bronze_can_promote=True`) and confirm `list_promotable_file_ingestions` returns `DatasetInjestion` DTOs with only those proposals. 4) Run `load_input_watermarks` and assert each string equals `DatasetIdentity.serialize_watermark`.  Suggested comment: Add a class-level comment describing `ops.file_ingestions` as the aggregate root and that `run_id`/`file_id` uniqueness prevents duplicates across Bronze runs.
- Observation: The UI-facing `DuckDbOpsRepo` mirrors the same aggregate but exposes read-only views (`RunOverviewRow`, `FileIngestionRow`) without introducing higher-level dependencies, keeping dependency direction downwards.  Evidence: Methods run SELECTs over `ops.file_ingestions`, `_fetch_dicts` returns dicts that DTO constructors consume, and no UI code references those DTOs directly.  Proposed tests: 1) Populate a temporary DB with two runs and ensure `list_run_overviews` returns the right aggregated metrics ordered by `started_at`. 2) Confirm `get_run_overview` returns `None` for a missing run_id. 3) Insert rows and ensure `list_file_ingestions` produces DTOs whose attributes equal the inserted columns.  Suggested comment: Document `_fetch_dicts` behavior so future maintainers know it assumes a cursor with `description` and that the repo is strictly read-only.
- Observation: `ChartSetJsonFileAdapter` and `AltairChartAdapter` protect the UI chart contract by insisting on JSON objects and by normalizing metadata before building layered Altair charts, so they serve as boundary keepers between UI DTOs and visualization primitives.  Evidence: `ChartSetJsonFileAdapter.read` raises when JSON root is not dict, `AltairChartAdapter` iterates `chart.series`, coerces metadata/axis/chart type, filters DataFrame rows, and supplies hover rules.  Proposed tests: 1) Feeding a JSON array to `ChartSetJsonFileAdapter.read` raises `ValueError`, and `write` creates the target directory when missing. 2) Provide a chart-like object with missing `series` and confirm `to_dataframe` returns an empty DataFrame or cleans NaNs when present, and that `to_altair` respects `date_range`. 3) Verify `to_altair` produces a layered chart that switches between line/area/bar depending on `chart_type` and optionally adds the hover rule when `add_hover_rule=True`.  Suggested comment: Add docstrings to `AltairChartAdapter` methods explaining that they normalize any incoming `Chart` object into the canonical dataframe structure before layering, so future contributors know where metadata conversions happen.
- Observation: `LoggerFactory` must recompute its log level for each call, deduplicate Stream/File handlers, and open the file immediately (no `delay`) so the new tests can assert INFO-vs-WARN behavior, handler counts, and the existence of `logs_<date>.txt`. Evidence: `src/data_layer/infra/logger.py`, `tests/unit/infra/test_logger_factory.py`.
- Observation: `DuckDbMigrationService` reads SQL files under `Folders.migration_absolute_path`; the bootstrap reported `FileNotFoundError` when the folder did not exist during simple unit tests, so the service now returns an empty list when the path is missing. Evidence: guard added to `_migration_files` in `src/data_layer/infra/duckdb/duckdb_migration_service.py`.
- Observation: The UI repo imports expect `models.ops_models` and a top-level `ui_settings`, so we added alias packages (`src/models/ops_models.py` and `src/ui_settings.py`) to re-export the existing `ui.models.ops_models` and `ui.ui_settings` implementations rather than refactoring callers. Evidence: new alias files plus passing UI repo tests.
- Observation: `ResultFileAdapter.read` sometimes hits `FMPResultMapper.from_serializable_dict` on payloads that lack a `request`, and the mapper expects attributes like `path`, `datatype`, and `hash`. To keep the adapter resilient, `RunResult` now explicitly declares those metadata fields before the mapper fills them. Evidence: `src/data_layer/run/dtos/run_result.py` and `tests/unit/infra/test_result_file_adapter.py`.

## Decision Log

- Decision: Treat the directory `src/*/infra` as the locus for infrastructure aggregates and value objects that must be reviewed before code changes.  Rationale: The user prompt and docs/AI_context/test_context.md describe these files as subject to the aggregate/invariant checklist, so isolating them avoids missing critical contract preservation.  Date/Author: 2026-01-27 / Codex.
- Decision: Recompute log levels on every `LoggerFactory.create_logger()` call, deduplicate stream/file handlers, and open the file immediately instead of delaying so tests can observe ENV switches and the existence of `logs_<date>.txt`.  Rationale: The infra tests require a WARN-level logger after toggling `ENV`, and assertions on handler counts fail because `FileHandler` inherits from `StreamHandler`. Updating the factory keeps the behavior deterministic for the suite.  Date/Author: 2026-01-27 / Codex.
- Decision: Add `src/models/ops_models.py` and `src/ui_settings.py` alias modules rather than refactor every UI import that currently points to `models.ops_models` or `ui_settings`.  Rationale: Keeping the existing import paths avoids touching higher-level UI services while still making the modules importable under pytest's `pythonpath=src`.  Date/Author: 2026-01-27 / Codex.
- Decision: Declare the metadata attributes (`ticker`, `path`, `datatype`, `date_key`, `error`, `filename`) on `RunResult` so `FMPResultMapper.from_serializable_dict` and `ResultFileAdapter.read` can hydrate partial payloads without `AttributeError`.  Rationale: When Bronze files lack a `request` block, the mapper still sets those properties; leaving them undefined crashes the reader.  Date/Author: 2026-01-27 / Codex.

## Outcomes & Retrospective

The DDD observations now drive concrete pytest modules, doc comments, and dependency aliases, and the entire infra suite described in `Concrete Steps` passes (the only noise was a DeprecationWarning from `datetime.utcnow()` used in the chart and repo tests). This means anyone can re-run the highlighted commands and expect the same guardrails to hold, and the living sections of this ExecPlan reflect the completed work so future Codex runs do not re-derive the same checkpoints.

## Context and Orientation

The repo contains several `infra` packages: `src/data_layer/infra` (dataset metadata, Bronze writers, ResultFileAdapter, logger, DuckDB bootstrap/migrations), `src/data_layer/ops/infra` (DuckDB repository for ingestion metadata), `src/ui/infra` plus `src/ui/charts/infra` (UI adapters and repositories). None of these modules import higher-level services or UI logic, so dependency direction consistently flows down toward folders/settings/dtos/duckdb. The ValueObjects/DTOs under review mirror the DDD checklist: `DatasetIdentity`, `DatasetWatermark`, `DatasetKeymapEntry`, `DatasetKeymap`, and `Migration` are immutable carriers, whereas `DatasetService`, `ResultFileAdapter`, `BronzeFileWriter`, the DuckDB repos, and logger/Altair adapters are aggregate or service layers that own or coordinate state transitions. Each observation above explicitly calls out the aggregate boundary, invariants, context-map constants, and entity/value-object classification defined in docs/AI_context/test_context.md.

## Plan of Work

First, replicate the observations from the Surprises section into test scaffolding: create `tests/unit/infra/test_dataset_service.py`, `test_dataset_value_objects.py`, `test_result_file_adapter.py`, and `test_duckdb_infra.py`, among others, so each invariant has a failing test until the code meets it. Second, add docstrings or short comments mentioned under "Suggested comment" to explain why those invariants exist (e.g., reminding maintainers of the Bronze append-only rule or SQLite table contract). Third, ensure the new tests exercise the context map constants (`DATASET_KEYMAP_FILENAME`, `Folders.*` helpers) by using temporary directories and environment overrides. Finally, run the infra test subset via `python -m pytest tests/unit/infra` and update the ExecPlan living sections once the tests pass.

## Concrete Steps

1. Install the chart/HTTP dependencies with `pip install httpx altair pandas` so the adapters and DTOs used by the new tests can import without errors.
2. From the repository root, run `python -m pytest tests/unit/infra/test_dataset_service.py tests/unit/infra/test_dataset_keymap_entry.py`; the suite now reports `7 passed`.
3. Run `python -m pytest tests/unit/infra/test_dataset_value_objects.py`; it now reports `2 passed`.
4. Execute `python -m pytest tests/unit/infra/test_result_file_adapter.py tests/unit/infra/test_bronze_file_writer.py`; both files now pass with `6 passed`.
5. Run `python -m pytest tests/unit/infra/test_logger_factory.py tests/unit/infra/test_duckdb_bootstrap.py tests/unit/infra/test_migrations_service.py`; expect `9 passed` once the logging/migration tests complete.
6. Run `python -m pytest tests/unit/infra/test_duckdb_ops_repo.py tests/unit/infra/test_ui_duckdb_ops_repo.py`; the combined suite now reports `7 passed`.
7. Finish with `python -m pytest tests/unit/infra/test_chart_adapters.py`; the command now reports `3 passed`.

Each command should exit with status 0 now that the dependencies, aliases, and code changes are in place; rerun only the keys that failed if future adjustments are required.

## Validation and Acceptance

Validation relied on running the listed pytest commands after installing `httpx`, `altair`, and `pandas`. Each suite now exits with status 0 and reports:

* `tests/unit/infra/test_dataset_service.py` + `test_dataset_keymap_entry.py`: `7 passed`
* `tests/unit/infra/test_dataset_value_objects.py`: `2 passed`
* `tests/unit/infra/test_result_file_adapter.py` + `test_bronze_file_writer.py`: `6 passed`
* `tests/unit/infra/test_logger_factory.py` + `test_duckdb_bootstrap.py` + `test_migrations_service.py`: `9 passed` (with a reminder about the new logger behavior).
* `tests/unit/infra/test_duckdb_ops_repo.py` + `test_ui_duckdb_ops_repo.py`: `7 passed` (one DeprecationWarning from `datetime.utcnow()` used in the UI test fixture).
* `tests/unit/infra/test_chart_adapters.py`: `3 passed`.

Acceptance is reached when these commands still pass end-to-end, the doc comments remain adjacent to the invariants they describe, and future edits re-run at least the subset that interacts with the touched modules.

## Idempotence and Recovery

Each test reads from temporary directories or in-memory DuckDB instances; rerunning them will reuse the same temp paths or reinitialize the in-memory store, so running the commands multiple times has no side effects beyond regenerating ephemeral files. If one test fails, rerun only that test file with the same command above, and rerun the entire `tests/unit/infra` set once the invariants are repaired.

## Artifacts and Notes

- The infra classes reviewed today span the following folders: `src/data_layer/infra`, `src/data_layer/ops/infra`, `src/ui/infra`, and `src/ui/charts/infra`.
- Commands used to inspect the files included `Get-ChildItem -Recurse -Filter *.py src | Where-Object { $_.FullName -match '\\infra\\' }`, which confirmed the 16 modules covered above.
- Future reviewers should keep this ExecPlan in sync with docs/AI_context/test_context.md anytime they add more `infra` classes or new invariants.
- Added alias layers (`src/models/ops_models.py`, `src/ui_settings.py`) and recorded the runtime dependency installs (`pip install httpx altair pandas`) so the UI modules and chart adapters continue to import without rewriting existing paths.

## Interfaces and Dependencies

- `data_layer.dataset.models.dataset_service.DatasetService` depends on `Folders.dataset_keymap_absolute_path`, `os.environ["DATASET_KEYMAP_FILENAME"]`, `yaml.safe_load`, `DatasetKeymap`, `DatasetKeymapEntry`, and `data_layer.dtos.dto_registry.DTO_REGISTRY`.
- `data_layer.dataset.models` value objects (`DatasetIdentity`, `DatasetWatermark`, `DatasetKeymapEntry`, `DatasetKeymap`) are consumed by the DuckDB repos and the Gold promotion pipeline, so their serialization must match `DatasetInjestion`.
- `data_layer.infra.result_file_adaptor.ResultFileAdapter` relies on `RunResult`, `RunRequest`, `DatasetRecipe`, and `Folders` plus the Bronze contract described in docs/AI_context/bronze_data_contracts.md.
- `data_layer.infra.bronze_file_writer.BronzeFileWriter` and `LoggerFactory` reference `Folders.data_absolute_path()` and `Folders.logs_absolute_path()` along with `settings` constants; they must stay at the bottom of the dependency graph.
- `data_layer.infra.duckdb` classes depend on `duckdb.connect`, `Folders.duckdb_absolute_path`, migration SQL files under `Folders.migration_absolute_path`, and share the `ops.schema_migrations` table.
- `data_layer.ops.infra.duckdb_ops_repo` and `ui.infra.duckdb_ops_repo` both depend on `DuckDbBootstrap`, but only the UI repo returns DTOs (`RunOverviewRow`, `FileIngestionRow`), so the UI layer does not leak dependencies into the data layer.
- `ui.charts.infra.chart_set_file_adaptor` depends on `shared.charts.domain.chart_set_dto.ChartSetDTO`, while `ui.infra.alt_air_adaptor.AltairChartAdapter` depends on `altair`, `pandas`, and chart metadata structures with `series`.
- `data_layer.ops.dtos.file_injestion.DatasetInjestion` imports `httpx` to describe how payloads flow through Bronze, so install `httpx` before running the new repo tests to satisfy the dependency.
