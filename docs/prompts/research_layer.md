Problem: research code tends to sprawl (feature logic mixed with notebooks, backtest assumptions scattered, results hard to reproduce). The fix is to treat “research” as a first-class layer with explicit inputs/outputs, plus a module layout that makes promotion into production straightforward and auditable.

Inputs and outputs

Inputs (what research consumes)

1. Gold warehouse datasets (clean, conformed, reusable)

* Prices (adjusted), volumes, corporate actions aligned, symbol/instrument identity resolved.
* Fundamentals, economics, technical base series in a consistent schema.
* Factor scores, macro regimes, risk metrics, engineered feature sets meant for modeling/consumption.
  This aligns with Gold responsibility: modeled metrics, aggregates, signals, forecasts. 

3. Research configuration (versioned)

* Universe definition (tickers, date range, sampling frequency).
* Strategy parameters (lookbacks, thresholds, rebalance cadence, costs, slippage model).
* Feature set selection and transforms.

4. Metadata / lineage

* run_id, dataset identifiers, as-of dates, code version, config hash.
  Your platform already treats run_id and metadata as first-class (especially at Bronze). Keep the same discipline in research outputs. 

Outputs (what research produces)

1. Research artifacts (the “truth” of what you tested)

* BacktestResult: performance series (equity curve), trades, positions, exposures, turnover, costs, slippage, drawdown, summary stats.
* Diagnostics: regime attribution, factor attribution (optional), parameter sensitivity, walk-forward summaries.

2. Persisted datasets (optional but recommended once stable)

* Gold-ready features/signals generated by research, written as versioned Gold outputs once validated.
* Monitoring metrics (PnL, exposures, model drift measures) that production can consume later.  

3. Visualization payloads for UI

* ChartSetDTO bundles (snapshot + charts) for Streamlit pages, so the UI doesn’t rebuild analytics on the fly. 

## Concrete folder / module layout (Strawberry-style)

Below assumes your existing top-level domains and medallion structure (company/economics/fundamentals/technicals/analytics/ui/shared). 

Repository layout (code)
strawberry/
analytics/
research/
**init**.py

```
  universes/
    __init__.py
    universe_dto.py          # universe definition (tickers, date span, freq)
    universe_builders.py     # S&P1500, sector slices, custom lists

  datasets/
    __init__.py
    loaders.py               # read Silver/Gold into DataFrames
    joins.py                 # canonical join keys and time alignment utilities
    adjustments.py           # split/dividend adjustment helpers (if not already upstream)

  features/
    __init__.py
    feature_spec.py          # declarative feature definitions (name, deps, transforms)
    transforms.py            # winsorize, zscore, rolling, lag, etc.
    builders/
      __init__.py
      technicals.py
      fundamentals.py
      macro.py

  signals/
    __init__.py
    signal_spec.py           # declarative signal definitions (inputs -> positions)
    rules/
      __init__.py
      momentum.py
      value.py
      quality.py
      regime_minsky.py       # if using the Minsky regime classifier conceptually

  costs/
    __init__.py
    slippage.py              # models (bps, sqrt(volume), spread-based)
    fees.py                  # commissions, borrow, funding

  portfolio/
    __init__.py
    sizing.py                # volatility targeting, risk parity, caps/floors
    constraints.py           # max position, sector caps, leverage limits
    rebalancing.py           # rebalance schedule + drift thresholds

  backtest/
    __init__.py
    engine.py                # event loop / vectorized runner
    broker_sim.py            # fill logic using costs models
    metrics.py               # sharpe, drawdown, turnover, hit-rate, etc.
    reporting.py             # assemble BacktestResult DTOs

  experiments/
    __init__.py
    experiment_spec.py       # ties universe + features + signal + portfolio + costs
    runner.py                # runs experiments, writes artifacts, returns results
    sweeps.py                # parameter grid / bayesian sweeps (optional)

  artifacts/
    __init__.py
    store.py                 # write/read artifacts (parquet/json) with run_id + config hash
    schemas.py               # artifact table schemas (trades, positions, curves)

  ui_adapters/
    __init__.py
    chartsets.py             # convert results -> ChartSetDTO for Streamlit

scoring/                      # existing scoring models can stay here
strategies/                   # production-ready strategies can live here
```

ui/
pages/
research_dashboard.py       # reads ChartSetDTO or Research artifacts; no heavy compute

shared/
dto/
config/
logging/

Data layout (files) for research outputs
Keep research outputs separate from medallion data, but follow the same partitioning discipline (dt/as_of/run_id). Your tech stack doc already standardizes Parquet and run_id correlation. 

/research/<experiment_id>/
as_of=YYYY-MM-DD/
run_id=<RUN_ID>/
config.json                  # full resolved experiment config (frozen)
summary.json                 # headline metrics + key flags
equity_curve.parquet         # timestamp, equity, returns
positions.parquet            # timestamp, symbol, weight, shares
trades.parquet               # timestamp, symbol, qty, price, costs
diagnostics.parquet          # turnover, exposure, drawdown series, etc.
charts.json                  # ChartSetDTO for UI (optional)

Minimal DTOs/interfaces to make this clean

1. ExperimentSpec (input)

* experiment_id
* universe (tickers/from/to/freq)
* feature_set (list of FeatureSpec ids)
* signal (SignalSpec id + params)
* portfolio construction params
* costs/slippage params
* execution assumptions (close-to-close, next open, etc.)

2. BacktestResult (output)

* run_id, experiment_id, as_of, code_version, config_hash
* equity_curve (series)
* positions (table)
* trades (table)
* summary metrics (dict)

3. ChartSetDTO (UI output)
   Use your existing ChartSet/Chart/Series contracts so research can feed Streamlit without bespoke plotting glue. 

How promotion works (practical rule)

* If a feature/signal is still experimental: it lives under analytics/research and outputs go under /research/.
* Once validated and intended for live use: move the signal computation into Gold (versioned datasets/signals), and have production consume only Gold outputs. This matches the Bronze→Silver→Gold separation you’ve already defined. 

If you want a next step that’s immediately implementable, I can provide: (a) a concrete ExperimentSpec dataclass and BacktestResult dataclass shape, (b) a “loaders + engine + artifacts store” minimal skeleton, and (c) a mapping from BacktestResult -> ChartSetDTO using your chart contracts.
